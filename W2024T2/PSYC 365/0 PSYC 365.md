[[syllabus_365_2024W-1.pdf]]

| PSYC_V 365-002 - Cognitive Neuroscience | Lecture | Tue Thu <br>3:30 p.m. - 5:00 p.m. <br>MATH-Room 100 | Brandon Forys | 2   |
| --------------------------------------- | ------- | --------------------------------------------------- | ------------- | --- |
### Readings and Assessments

| 1   | 01.07.25 | Introduction/Syllabus Module Syllabus                                                                                               |
| --- | -------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| 2   | 01.09.25 | Cognitive Neuroscience: The Good, the Bad, and the Ugly Passingham Chap 1; How to Read a Scientific Paper for Non-Scientists        |
| 3   | 01.14.25 | Quiz: Cognitive Neuroscience methods Neuroanatomy module; Poldrack & Farah, 2015                                                    |
| 4   | 01.16.25 | fMRI: Workhorse of Cog Neuro Dimsdale-Zucker et al., 2018, section 1 only                                                           |
| 5   | 01.21.25 | Recognizing objects Part 1 Passingham Chap. 2 to pg. 58 (Classifying Objects); Brain facts: How AI helps us understand human vision |
| 6   | 01.23.25 | Recognizing Objects Part 2 Neural networks made easy; Bowers et al., 2023 (pgs. 1-19)                                               |
| 7   | 01.28.25 | What is special about faces? Harada et al., 2020; Cracking face code                                                                |
| 8   | 01.30.25 | Review                                                                                                                              |
| 9   | 02.04.25 | **Midterm 1**                                                                                                                       |
| 10  | 02.06.25 | Guest lecture (to be announced) TBD                                                                                                 |
| 11  | 02.11.25 | Classifying objects Passingham Chap. 2 pg. 58 (Classifying Objects)                                                                 |
| 12  | 02.13.25 | Predicting Perception Egner, Monti, and Summerfield, 2009 17-21 Feb NO CLASS – Reading Week                                         |
| 13  | 02.25.25 | Selecting attention Passingham Chap. 3; Videos                                                                                      |
| 14  | 02.27.25 | Sustaining attention Rosenberg et al., 2016                                                                                         |
| 15  | 03.04.25 | Emotion, motivation and attention Inman et al., 2023                                                                                |
| 16  | 03.06.25 | Review                                                                                                                              |
| 17  | 03.11.25 | **Midterm 2**                                                                                                                       |
| 18  | 03.13.25 | Reward and attention (& learning) Anderson et al., 2016; 2 Dopamine blogs                                                           |
| 19  | 03.18.25 | The Hippocampus: From space travel to time travel Passingham Chapter 4; Fernandez- Velasco & Spiers, 2024                           |
| 20  | 03.20.25 | Episodic memory How to See a Memory                                                                                                 |
| 21  | 03.25.25 | Rehearsing and retrieving memories Bird et al., 2015                                                                                |
| 22  | 03.27.25 | **Midterm 3**                                                                                                                       |
| 23  | 04.01.25 | Final projects                                                                                                                      |
| 24  | 04.03.25 | Final projects                                                                                                                      |
| 25  | 04.08.25 | Final projects                                                                                                                      |




# 01.07.25 Lecture 1
[[Class1_365_2025.pdf]]
Understanding relationship between brain activity and cognitive processes

# 01.09.25 Lecture 2
[[Class2_365_2025.pdf]]
[[Raff How to read and understand a scientific paper a guide for non-scientists|Raff how to read]]
[[Cognitive Neuroscience_ A Very Short Introduction-Passingham.pdf|Passingham Ch2]]
[[Class3_365_2025.pdf]]

# 01.30.25 Lecture 8

**SHORT ANSWER QUESTION**  
Answer the following based on the paper we read by Harada et al., 2020, which used fMRI to examine  
questions related to face perception and race/culture, as well what you know from lecture material  
about the paper (30 points).  

- **A. What was the big-picture question that motivated the study? (1 pt total)**  

- **B. Background (6 pts total)**  
	- *I. Describe one thing that previous research, described in the Introduction of the paper, has taught us about the influence of culture on responses to facial emotion and one set of findings described in the paper that support that knowledge. Your answer for the  second part of this question (about the set of findings) should elaborate on your answer  in the first part of this question. (4 pts)* 
	- Previous studies have shown in-group affects on bilateral amygdala response. 
	- *II. What two self-construal styles have been identified by cultural psychologists and what are the main characteristics of each? (2 pts)*  
- **C. Research Questions and Hypotheses (3 pts total)**  
	- *I. What was the main research question? (1 pt)*  
	- How do Japanese bicultural individuals (raised in individualistic culture but with connections to collectivistic) compare to Native Japanese (collectivism) in amygdala response to racial in and out group faces? ("That is to say, what neural modulation in the amygdala would be observed in Japanese-American individuals, who are very familiar with Caucasian-Americans’ faces while they have a similarity of physical appearance to native Japanese people, when they were shown facial emotional ex- pressions of Caucasians and Japanese?")
	- *II. What were two hypotheses put forward by the authors? (2 pts)*  
	-  Bicultural (Japanese American) individuals would show greater amygdala response to Japanese faces than Caucasian Americans, and "collectivistic tendencies would be related to neural responses of intergroup negative facial expression."
- **D. Methods (2 pts total)**  
	- *I. How did they define group membership? (1 pt)*  
	- Japanese-Americans were born and/or raised from a young age in America, either first-generation or second-generation. Japanese Natives were born and raised in Japan, and Caucasian Americans. In-group for Japanese Americans was racial in group ie. Japanese. 
	- *II. What was the purpose of the shape stimuli? (1 pt)*  
	- The shape stimuli serve as a control task for a "relatively low-level cognitive process"  
- **E. Results (4 pts total)**  
	- *I. What was the main pattern of findings for the amygdala? What patterns of brain  activity reflected the “cultural in-group effect?” (2 pts)*  
	- There was a significant in-group effect observed in the bilateral amygdala when comparing Native Japanese group with Caucasian-American group. Native Japanese participants showed larger neural responses itn the bilateral amygdala to Japanese faces than Caucasian-Americans did. 
	- Caucasian-American and Japanese participants showed the highest amygdala activities to negative faces of the same cultural and racial members (i.e. in-group members) and the lowest amygdala activities to those of the different cultural and racial group members
	- Japanese-American participants showed neural responses in between the JN and CA groups. 
	- *II. What pattern of brain activity did they find in one particular group of participants when  they looked at the whole brain? (2 pts)*  
	-  Japanese-American participants showed greater activities in the right ventral prefrontal cortex, posterior cingulate cortex extending to precueus and right superior frontal gyrus during processing of negative facial expressions of Japanese than Caucasian-American participants did
- **F. Discussion (4 pts total)**  
	- *I. How do they interpret the midline cortical findings observed in one particular group? (2  pts)*  
	- Likely reflects greater informational demands of self-relevant memory, because their recognition of their racial identity might be automatically strengthened when negative facial expression of racially in-group members was presented.
	- *II. Was their hypothesis about individual differences supported? Why or why not? (2 pts)*  
	- "This result indicates that bicultural individuals respond to negative facial expressions of members from the both cultural groups with slightly attenuated extent as compared to monocultural individuals. We hypothesized that JA individuals would show large amygdala responses to Caucasian negative faces as the same extent as CA individuals would if the in-group bias was due to familiarity of faces learned in their daily life, while they would show large amygdala response to Japanese negative faces as the same extent as JP individuals would if the in-group bias was simply due to their racial identity. The results in the current study did not support either hypothesis"
	- "Although this way might not necessarily eliminate the possibility of remaining variations, it would be still a possible solution and even more useful way by combining with an additional and detailed reliability test such as a ROI analysis for target brain regions that we have done in the current study. In the current study, we excluded the temporal and occipital regions from the analyses only when directly comparing the different facilities, and furthermore confirmed that there was no significant difference of neural activities in the hypothesized regions, that is the bilateral amygdala, in our prior reliability test. Hence, variation in scanner-site performance is not a likely explanation for the variability in the neural activation we observed in the current study."
- **G. Conclusions and limitations (6 pts total)**  
	- *I. How do they describe the overall meaning and importance of the results? (2 pts)*  
	- "Our results show that neural responses in the bilateral amygdala reflected both of in-group biases based on ones’ racial identities and cultural practices, such as ones’ collectivistic tendencies, irrespective of participants’ cultures."
	- "Our results demonstrate that neural responses during the processing of emotional faces could be modulated by different social factors, such as cultural practice and racial identity, and furthermore cultural and racial influences are interlaced especially in bicultural individuals such as Japanese-Americans in the current study."
	- *II. List TWO limitations of the study and explain why they are limitations. (4 pts)*  
	- One limitation of this study involves the generalizability of our findings, as the results were based on only facial expressions of fear and anger.
	- There might be another limitation in the current study, that is a possible variation in the scanner performance between the two facilities which may have led to variations in the neural activation patterns across the two different scanning sites.
- **H. In YOUR opinion (4 pts total)**  
	- *I. What are the larger cultural or societal implications of these findings? Do you agree  with the authors’ conclusions? Why or why not? What do you think is important that  they didn’t think of? (4 pts)*

# Midterm 2

## Class 10 - Guest Lecture

[[Class10_psyc365_guestlecture.pdf]]

> [!note] Suggestion –
> Focus on the descriptions of the tasks used and the key takeaways given for each study. Questions about this guest lecture would appear in the multiple choice section only.

### Study – Reward sensitivity, facial emotion recognition, self-referential processing in MDD & BSD
- **Aim**: Reward sensitivity, facial emotion judgement, self-referential processing in acutely depressed people with MDD + BSD vs healthy control
- **Hypotheses**: 
	- Reward sensitivity: MDD < BSD < CTL 
	- Facial emotion recognition: MDD > BSD 
	- Self-referential processing: MDD = BSD > CTL
##### Monetary incentive delay task
Earning tickets towards draw for $100 prize when they reacted quickly enough (pressing space bar to smily face)
Asked how excited they were about how many tickets they were playing for (# varied)

##### Facial emotion labeling task
Morphed facial continuum from sad to happy 
Shift point: where on continuum participant shifted from identifying face as happy/sad
Slope: rate at which response shifted from happy to sad 

##### Self-referential encoding and memory task
Presented with words descrbiing personal characteristics/trait
Positive/negative self-referential encoding - how many pos/neg words participants chose as describing self

Memory bias: asked after a time what words they could remmeber from the lsit, measuring if they remembered more neg or pos words

#### Results & Key takeaways
**Results:**
- Individuals with MDD had sig. lower anticipatory reward sensitivity than BSD participants (p = .006, d = .73) and control participants (p < .001, d = 1.14)  Participants with BSD did not differ from controls in anticipatory reward sensitivity (p = .454)

- MDD/BSD/CTL (control) groups did not have significant differences in labelling faces as sad vs. happy or sensitivity to the changes in expressed emotions
- Individuals with MDD endorsed sig. fewer positive traits than the BSD (p = .002, d = .80) and CTL groups (p < .001, d = 2.01) 
- Individuals with BSD endorsed sig. fewer positive traits compared to CTLs (p <.001, d = 1.11) 
- MDD and BSD did not differ sig. for number of negative traits (p = .535) or negative self-referential memory bias (p = .311)
**Conclusions:**
Ppl w BSD have more anticipatory reward sensitivity & positive self-referential encoding 
→ May help with ddx-- identifying MDD vs BSD

### Study – Cognitive-Affective processes heterogeneity in MDD & BSD
- Background: Diff.s in cogn-affective processes exist among currently-depressed ppl w/ MDD vs BSD
	- As a group, BSD ascribe more pos traits compared to MDD/CTLs, & have greater anticipatory reward sensitivity 
- **Research question**: How much heterogeneity is there among BSD? – better, more personalized treatment
	- Why might there be heterogeneity? 
		- Depressive episodes can look diff 
		- Prev research has explored that clusters of cogn. affective processing across mood spectrum include participants who are not acutely depressed – these studies looked primarily at facial recognition tasks 
- **Aim**: To identify data-driven subgroups based on cogn-affective processing amongst acutely depressed indiv w/ MDD & BSD

##### Monetary Incentive Delay Task
##### Self-referential Encoding and Memory task

##### Analyses
1. Identify clusters/sub-groups
2. Assess cluster diff in task performance & clinical/demographic var. 
3. Assess proportion of diagnoses across clusters

#### Results
![[Class10_psyc365_guestlecture.pdf#page=22|Class10_psyc365_guestlecture, page 22]]
**Cluster 1: "Negative-Low-Rewarders"**
- Significantly lower anticipatory reward sensitivity & positive self referential encoding, & higher negative memory bias & negative self ref
**Cluster 2: "Positive-High-Rewarders"**

Both clusters scored equiv. on depression matrix

**BSD vs MDD**
More MDD participants were NLRs 
No sign. diff in BSD participants who were NLR/PHR

More controls were PHRs

#### Key takeaways
- NLR & PHR represent distinct cogn-affective processing subgroups 
- There is heterogeneity in cogn-affective processes across mood spectrum 
- Some patients (MDD, BSD) cluster together w/ most healthy controls

*→ Personalized treatment is important*


### Study – Attentional biases in Persistent Post-Concussion Symptoms (PPCS)
Psychosocial factors greater contributers to developing PPCS than injury characteristics
Overlap with chronic pain research
- **Aims**: 
	- To describe correlations between attentional biases as measured on the attentional blink task (in terms of difficulty disengaging from pain-related stimuli), and fear-avoidance model constructs. 
	- To describe correlations between attentional biases as measured on the gaze-time task (in terms of preferential looking towards symptom- relevant stimuli), and fear-avoidance model constructs.
- **Hypotheses**: 
	- Participants who demonstrate greater attentional biases in difficulty disengaging attention from pain-related stimuli will also report greater severity of the fear-avoidance model contructs 
	- Participants who spend more time fixating attention on symptom- relevant stimuli will also report greater severity of the fear-avoidance model constructs (i.e., symptom severity, pain-catastrophizing, and fear- avoidance behaviour)

*Popular ways of measuring attntl biases: emotional stroop, dot probe*, but - poor split test reliability, test-retest, internal consistency, also, had to use tasks that didn't rely on reaction time, since this can also be affected post-concussion

**Groups**: 
- PPCS 
- Recovered
	- Both w/in 1 month of concussion 
##### Attentional blink task
**Standard**
- RSVP stream = rapid serial visual presentation stream
- Target 1 (T1): Letter in white that participant needs to ID
- T2: Number that participant needs to ID
- Distractors: Letters between
- Lag: Distance between targets
![[Screenshot 2025-02-27 at 1.17.42 PM.png|300]]
- Healthy participants (black line) show "attnl blink": If T2 is presented 200-400 ms after T1, we have greater difficulty IDing – theory, limitation in brains resources to ID target after just IDing
- *Hypothesis*: PPCS blue line
**Study**
Pictures instead of words for eco validity
T1: pain or neutral face
T2: bird,flower,furniture
Distractors: scrambled/pixels

##### Gaze/eye tracking task
Simulates eye-tracking w/ mouse movement
Side by side images with overlay
a) Neutral, General threat, concussion threat - b) Neutral 
general threat = natural disaster, fire, concussion threat = someone getting hit in head, MRI

#### Results - not sig. 
No diff in attnl blink for neutral or pain face
As expected: saw attnl blink for PPCS + recovered for pain faces, but did not see any diff btwn groups 
Sensitivity analysis: regrouped by fear avoidance rather than PPCS/recovered, still no sig. results

**Why not?**
- Task not sensitive enough
- Concussion images not threatening 
- Participants too removed from injury 
- Pain faces insufficiently relevant
- More complex processes

*Barriers to the implementation of new cogn paradigms into clinical practice*
- psychometric prop. 
- determining relative util
- disciriminant and predictive val
- cooperation across fields

## Class 11 - Classifying Obj and predicting perception

> [!example] Today:
> - Passingham Ch. 2
> - Selecting attn. 
> - Stim similarity in LOC
> - Reading: Egner, Monti, Summerfield 2009 [[Egner_Monti_Summerfield_09.pdf]]

[[Class11_365_2025_notes.pdf]]
> [!Important] Content to review (from quizzes)
> - Contents of Connolly et al
> - "Which type of category would be more likely to be represented in patterns of more or less activation across voxels (multivariate patterns) than by preferential activation of whole regions in the ventral visual stream?" -- Animal classes


***Review***
- Preference in LOC for whole obj 
- Adaptive suppression in LOC for diff angles of obj

*Discussion*
- *Difference between recogn an obj and making sense of it?*
- *How is classifying an obj a form of making sense of it?*

### Inferotemporal cortex/Ventral visual system (IT)
- Important for obj categorization – *semantic* meaning of obj

Ventral visual sys (IT) categorizes obj at diff lvls:
- Superordinate, basic, subordinate, exemplar
	- Obama ex.: animate/face/man/president

#### Encoding approaches: divisions and hierarchies
Superordinate → exemplar: largest → smallest brain area (>1cm → < mm)

> It is not enough simply to recognize what we see; we also have to make sense of it. This involves classifying things, whether animate or inanimate... 
> *Passingham, Ch. 2*



##### Connolly et al: Categorizing bugs, birds & mammals 
- How are finer grained distinctions rep? 
- Prev research: funcitonal landmarks (*Animate/inanimate distinctions in IT- Grill-Spector*: Diff areas light up)
- Less known about finer distinctions
	- Diff classes of animal
- **Goal**: look at pattterns across voxels (**RSA**) to look for *similarity structures* at the level of biological class

*RSA review*
- Comparing patterns of representation, shared representational space

**fMRI experimental design**
- *Simple recognition memory task
- *Rate stimulus simularity*

"12 young adults participated in an fMRI study. As they lay there in the scanner in each trial they’d see 6 different series of 3 pictures of animals of the same class of animal -- either bugs or birds or primates. Then they would be shown one of those 6 series of 3 stimuli and would have to say if it was the same or different from what they saw in the previous trial. This probe question was just to keep the participants paying attention -- the experimenters didn't really care about how well they remembered the items. Here are some examples of the stimuli which were either bugs, birds & primates. *1) Afterwards they had participants rate how similar all of the different stimuli were to each other* and also do a task where they’d *2) see three and choose the one that wasn’t categorically like the others*. In this case they did care about the participants' choices because they wanted to know how similar or different they thought the different stimuli were from each other."

**RSA Results: LOC & IT**
- Pattern of activity represents stimuli matching behavioral similarity ratings &  mirroring biological class structure

***→ Human neuroimaging reveals a "hierarchical category structure that mirrors biological class strucutre" in ventral visual stream***

###### Using encoding for brain mapping in LOC
- How does abstr rep of continuum from bug to primate literally map to brain space 
- Does map for primates vs bugs look like animate vs inanimate? 
- Lateral-medial org. 

![[Screenshot 2025-02-27 at 1.53.36 PM.png|300]]

***→ Primate vs bug very similar to animate vs inanimate*** 

**Brain maps categories from inanimate to animate**
- Animal categories rep. medial (inanimate) → lateral (animate)
	- Only one dimension of rep obj
- Using behav judgements as target, semantic structure–ways of categorizing animals–reflected strongly thruout LOC: how much "like us" an animal is maps on to medial-lateral
	- Formed early in development, kept into late dementia

#### What we have learned from fMRI

- *Encoding Approach*: Responses to object parts and then whole objects along moving along occipital cortex from EV to LOC 
- *Encoding Approach*: More invariant processing and more category specificity as we move along ventral stream. 
- *Decoding approach*: shows continuum of finer-grained categories. 
	- In the ventral stream it matches semantic judgments of animal class 
	- In LOC it is organized along a spectrum of inanimate (not like me!) to very animate (a lot like me!)
### Predictive Coding
*Intro: waking up from anaesthetic TEDx*
- Consciousness, animal consciousness, computer consciousness
- World vs self consciousness 
- Top-down/bottom-up processing
	- Predictive hallucinations

**The problems of perception**:
- Diff objects can produce same images (e.g. orange vs orange ball)
- Same obj can produce diff images (e.g. diff angles)

*Review: Feedforward Feature Based Models*
- = Bottom-up 

→ How do we decide what to interpret an ambiguous image as? Between hypotheses H1/H2, which both equally acccount for lower-lvl features? = **Top down info**

*von Helmhotz*: First to describe brain as "prediction machine"
Perception as unconscious inference
- Cause of sensation inferred via its effects

##### Step by step of predictive coding model

![[Screenshot 2025-02-27 at 2.52.33 PM.png]]
 
*Prediction error*
- At each lvl there are "representational units" that encode expectation: probability of given stim under circumstance/conditional probability
- Send predictions to lower level for what they expect to receive
- "Error units" encode surprise: mismatch between prediction & bottom up info 
- Send mismatch info/prediction error upwards to revise hypothesis until prediction error is minimized

*Reading question:* How do predictive coding  models view the role of visual cortex neurons? 
 - Predictions based on the probability the stimulus will have particular features 
 - Error detection — they respond to a mismatch between predicted signal and actual signa
[[Class11_365_2025_notes.pdf#page=47&selection=4,0,4,16|End here]]





## Class 12 - Predictive perception, Egner et al
[[Class12_365_2025_notes-2.pdf]]

> [!example] Today:
> Reading: [[Egner et al. - 2010 - Expectation and Surprise Determine Neural Population Responses in the Ventral Visual Stream.pdf]]

[Recording](https://ubc.ca.panopto.com/Panopto/Pages/Viewer.aspx?id=bebc1490-63d8-465f-af5b-b25f0138c91a)

### Enger et al - 2010: Predictive Coding & Surprise
**Big picture question:** Do predictive coding models explain visual object recognition better than classic hierarchical feature-based models? 
	→ Examine by taking advantage of what we know about category selective voxels in fusiform face area (FFA) and parahippocampal place area (PPA)

**Predictive coding:**
- Perception is *inference*
- 2 processing units at every level of vis hierarchy
	- Representation (expectation)
	- Error (mismatch between expectation and raw evidence/surprise)
	- *Units:* populations of neurons, aka voxels in fMRI 

**Feature detection:**
- Visual neurons respond to *features* of an object
	- eg. FFA neurons respond to eyes, facial configuration, etc. 

**Research Question:** Does BOLD activity in the FFA reflect responses to expectation + surprise? Or just face features? 
**General Hypothesis:** FFA activity will be an “additive function” of expectation and surprise. 
**Alternative hypothesis:** There will always be more FFA activation to faces 
- Expectation and surprise will not matter!

- Who were the participants?: Young-ish college students w good vision
- How many of them were there?: 16? 

**Independent var:** 
- Target/non-target
- Stimulus probability (% of time face vs house)
- Stimulus feature (face/house)
**Dependent var:** 
- Reaction time
- BOLD response in FFA and PPA

**Predicted results:** 
![[Screenshot 2025-03-03 at 11.44.24 AM.png|300]]
- FFA will still activate for expectation of seeing face, not only actually seeing face 
- Additive result = face expectation + face surprise = predictive coding

**Specific hypotheses:**
- *Predictive coding:* FFA responses to faces and houses should be most different when face expectation is low
- *Feature detection:* FFA responses to faces should always be greater than hosues regardless of the level of expectation

#### Results
**Reading question**
What were the behavral (reaction time) results
Main effect: faster to ID upside down faces than houses
Did not vary by expectation condition and there was no interaciton between stim type and expectation condition 

→ if participants were paying more attn to face features in the high face expt cond then their RTs should be faster. Since they aren't, the expectation manip is probably not infl attn: Accuracy was at ceiling -- easy task 

![[Screenshot 2025-03-03 at 11.54.45 AM.png|450]]

Greatest difference in the low expectation condition 
→ high expectation: virtually identical activation between face and house

**Strongest piece of evidence:** 
PC effect was not high for houses, but when chance of seeeing face was high and house was shown, there was signf FFA actvn

Best fit PC model: 2:1 surprise/expectation effect on actvn 

*Addtl models tested:*
- Baseline shift model, addtve
- Contrast gain model, multpv

Why did they also analyze parahipp place area as test of whether FFA results generalized? → control, *to see if generalizes to other brain areas– is FFA unique in its PC?* No, it does generalize to parahipp place area

**Critiques?**
Key – we know more about relative contributions of prediction and error units. Other things in discussion – there might be an interaction with attention if that was relevant to the task. And we don’t know how much the BOLD response reflects top down vs. bottom up inputs.

Problem with computational modeling – number of parameters, parsimony, not overfitting

**Conclusions:**
- Pattern of results in FFA was consistent w response that added expectation and surprise and with predictions of computational models based on predictive coding assumptions
- They conclude → *Prediction coding models describe the process of visual inference better than feature detection* 
- Encoding predicition and error is a general characteristic of how the brain works

Minimizing prediction error; 
- Bottom up signal sends forward prediction error only– not actual signal, but when mismatch occurs 
	- Features match expectations propogate upward– ? **what did he mean by this...
- Top down signal is representation/model of the world, generating predictions at all levels of visl hierarchy– "explain away"

### Attention
Many different forms of attention
We are focusing on **selective attention** and **sustained attention**

**Monkey business illusion**
- Too many people know about the gorilla trick–focus on the gorilla, miss that the curtain also changed color, a player left the team!

##### Overview

**Selective attention**
1. Identifying targets
2. Classic model: Top down and bottom up
	1. Attl sets in top down attn
3. Dorsal (top down) and ventral (bottom up) attn networks

**Sustained attn**

Top down and bottom up are not the same in attn as obeject recognition
dorsal ventral not same as vis streams
##### Overt versus Covert attention
**Overt:** with the eyes (looking at object of attention)
**Covert:** with the mind (obj of attn is in peripheral: attend but not looking directly)
[[Class12_365_2025_notes-2.pdf#page=33&selection=2,0,2,40|Class12_365_2025_notes-2, page 33]]
1:05

#### Selective attention
** Review passingham: referenced thruout lecture

- When you filter the world so that you attend to what’s important and ignore what isn’t ie cracks in sidewalk vs whole road
- But what determines what’s important?
	- Relevance to goals 
	- “Grabbiness” (Boston Pizza, shiny things)
	- Other kinds of relevance?

##### 2 attention systems: Top down/Bottom up
***Top Down (controlled)*** 
- Deliberate 
- Conscious 
- Goal-directed (task-based) 
- *Attentional sets* 
	- Mental templates that allow us to selectively attend to a certain category of stimulus before it appears (color, shape, size) 
	- Involve holding in mind features or location of the object you’re expecting

***Bottom Up (automatic)*** 
- Involuntary 
- Captures attention 
- Despite our goals 
- Feature-based
- Can miss gradual changes 

##### Dorsal (DAN) and Ventral (VAN) Attentional Networks
![[Screenshot 2025-03-04 at 1.32.02 PM.png|450]]
- FEF= Frontal eye fields: often attune perception to area of world
 - IPS = Intraparietal Sulcus: interaction between interpretive parts of brain such as exec function
 - VFC = Ventral frontal cortex approach PFC; decision making, setting up attnl sets
 - TPJ = temporoparietal junction: similar to intraparietal sulcus 
 - V = Visual cortex

*Biased competition*: A neural mechanism for selective attention
- ie attending to stripes looking for waldo, finding some other stripy thing, bringing attention back away from it to keep searching 

Frontal and Parietal regions *modulate* activity in visual cortex

##### Interim summary 
- Selective attention is the form of attention that allows us to select what is relevant and ignore what isn’t.
- In classic models, attention can be feature-based (bottom-up), or task-based (top down) 
- These types of attention are (mostly) mediated by VAN and DAN

Simpson clip of homer getting distracted from important point

#### Sustained attention 
*What is sustained attention?* 
-  Staying on task, even when boring 
-  Often measured by the Sustained Attention to Response Task (SART) 
-  Individual differences associated with ADHD, impulsivity

*Difficulties in sustained attention*  
- Easily distracted by non-relevant stimuli  
- Often forgetful in daily activities  
- Difficulty sustaining attention during activities  
- Difficulty following instructions/failing to complete tasks  
- Missing details/making mistakes 
- Avoidance of activities that require sustained mental effort


## Class 13 - Rosenberg, Class discussion
[[Class13_365_2025_notes-1.pdf]]
[[Rosenberg_etal_2016.pdf]]
[[Tips for reading Rosenberg et al., 2016.pdf]]

**Learning Objective**  
- Confidently describe recent research defining a neural marker of *sustained attention* and its relationship to ADHD

##### **Goal** 
Identify a neuromarker of sustained attention based on whole brain *intrinsic connectivity*
- Strength of using resting state data? -- straight-forward to collect and share across locations, cultural, language barriers; generalizable; works for participants like young children who aren't good with sustained attn task

##### **3 assumptions**
1. Individ diff in sustained attn will be reflected in complex patterns of correlated bold activity across brain reg
2. These patterns will be obsvd both when doing task and at rest
3. If we can distill a signature of these patterns we should be able to use them to predict attn ability in a completely diff set of ppl

Methods still being used today, incl. beyond sustained attn, very widely cited paper
*→ using brain connectivity measures as neuromarkers of clinical conditions*

##### **Questions to be answered:**
-  Big picture question 
	-  Can we find a neuromarker, or brain-based measure of general attentional ability? 
-  More specifically…
	-  ISO reliable marker of *overall attentional ability* observable in *resting* brains and *generalizable* to new populations 
-  **Specific question** 
	-  Can we get take a *data-driven* approach to pulling out patterns of network activity to give us a marker of sustained attention that will generalize across populations?

*What do they mean by a data-driven approach?*
- Data driven is an *exploratory* method. Instead of going in with very clear hypotheses (e.g., I think THESE regions will predict attention ability and ADHD symptoms) you measure brain activity when people are paying attention and see what the data tells YOU!. And of course they chose to measure sustained attention because it’s linked to ADHD.

Sustained attn task; fading between cities and mountains; hold button down when mountain; 90% of time city (require closer attn); higher error rate w lower sustained attn

##### Methods
-  Yale participants: 25 students 
	-  gradCPT task 
		-  Performance measure: $d^i$ or sensitivity = hits–false alarms 
			-  How accurate is each person taking into account tendency to just hit the button 
		-  fMRI collected while they were performing the task 
	-  Resting state fMRI data 
-  Beijing participants: 113 kids & teenagers with ADHD and typical controls (mean age 11 years) 
	-  Resting state fMRI data 
	-  ADHD-RS scores (inattention & hyperactivity/impulsivity)

*Generalizations*: from Yale to beijing, from adult to child, from attn network to ADHD scores

##### Variables
**Independent/Predictor** 
-  gradCPT conditions: Go vs. Nogo (city vs mountain) 
-  Choice of 268 network nodes 
-  Participant group (neurotypical adult, neurotypical youth, ADHD youth) 
-  Summary measures of network connectivity based on BOLD time courses 
	- Positive network strength 
	- Negative network strength 

**Dependent/Outcome** 
-  BOLD response 
-  Performance on gradCPT ($d^i$) 
-  ADHD-RS Score  (parent's opinion of attnl ability)
-  Summary measures of network connectivity based on BOLD time courses 
	-  Positive network strength 
	-  Negative network strength

Variables operationalize cognitive processes 27/2/25 PSYC 365: Class 14 17 -  $d^i$ (performance on gradCPT) -  ADHD-RS Score -  SAN models: Built from correlations between BOLD time courses -  Capacity for sustained attention -  ADHD symptoms (associated with problems with sustained attention) -  Neuromarkers of high and low levels of sustained attention ability

#### **3 step analysis**
1. Derive a network of regions whose connectivity strength predicts gradCPT task {behavioral measure} performance ($d^i$) in Yale group 
2. See if stat. model based on net strength while performing gradCPT can be applied to resting-state data to predict gradCPT task performance {networks that fire together at rest tend to fire together at work}
3. See if SAN model can be used with resting state data to predict ADHD scores

*What is Sustained Attention Network (SAN) model and how was it derived?*
-  The SAN model is a brain network based statistical model 
	-  You take correlated activity across a bunch of regions across the brain and reduce it to two numbers (summary statistics) 
	-  Those numbers reflect degree of connectivity in a brain network associated with the capacity for sustained attention 
	-  SAN model uses brain network scores to predict an individual’s attention performance

*How do we get these two numbers?*
##### Step 1
Assess what patterns of brain connectivity are relevant to attention 
	I. Measure correlations between pairs of nodes in each participant 
	II. Test which patterns of connection between regions is linked to good and bad gradCPT performance between participants

**I. Measuring correlations**
Choosing nodes (regions) across canonical networks
- Node = Avg all voxels in brain region →  see how BOLD incr. & decr. 
- Each edge between nodes is thin/thick depending on the level of correlation
- Matrix of correlations: Arrange edge strengths–temporal correlations–into matrix. 

**II. Patterns of connection** 
Correlate edge strength with good and bad gradCPT performance 
-  Positive tail: Edges most correlated with good performance (757 edges) – it’s a network. 
-  Negative tail: Edges most correlated with bad performance (630 edges) – it’s a network 
-  Network strength: *Summary statistic* calculated for each tail by adding up all of the correlations
	- Sum up corrl of all edges in each tail – pos network strength → high attn, vice versa
	- Both measures of network strength corrl w $d^i$

SAN model = model of relationship btwn network strength and $d^i$ scores 

Cross validation = taking one participant out, calculating best prediction w/o them, using groups of edges to calculate summary score for left out person, then using regression model to predict all but left out person's $d^i$.

**Result of cross-validation:**
Both pos and neg networks highly corrl w/ gradCPT performance (r>.8)
- Internal validation: prediction from task connectivity


##### Step 2
- See if SAN model, based on brain networks identified during performance of the gradCPT task, can predict gradCPT task performance from tail scores calculated from *resting state* data in the same people

**Correlation using resting state data**
![[Screenshot 2025-03-04 at 4.50.19 PM.png|300]]
- Controlling for age, IQ, and hyperaactivity 
- Not as high as gradCPT, but still good: r>.4
##### Step 3
- See if you can use the SAN model (created from Yale data during gradCPT task) to predict ADHD scores in kids and adolescents in Beijing based on network scores calculated from their resting state data.

**Correlation between SAN and ADHD score**
![[Screenshot 2025-03-04 at 4.52.21 PM.png]]
- Negative corrl between sustained attention - $d^i$ score - and ADHD score
- Prediction works!

![[Screenshot 2025-03-04 at 4.53.34 PM.png]]
*High Attention Network*  
- connections between motor & occipital cortex & cerebellum 
- connections between subcortical/cerebellar, motor and visual *networks* 
*Low Attention Network*
-  connections between temporal and parietal regions & between hemispheres in temporal and cerebellar regions 
- connections within subcortical and between subcortical and frontoparietal *networks*

#### Rosenberg Summary 
- SAN model allows functional connectivity between many nodes from many canonical networks to predict cognitive ability across different populations 
- “Holistic neural index of sustained attention" 
- Benefits of resting state data 
- Can be collected quickly and easily 
- Predictive not just descriptive 
- Not confounded by differences in task performance due to age or training 
- Meaningful link between ADHD and sustained attention 
- Does NOT suggest that attention is a unitary process!

- Sustained attention emerges from coordinated activity all across the cortex, subcortical structures and the cerebellum 
- Underlying networks extend far beyond traditional regions and networks 
- Serves one of the main goals of human neuroimaging: 
	- Identify *neuromarkers* that predict educational or health outcomes 
- Results suggest ADHD is a continuum of neural and behavioural function


> [!note] Clarifying notes from Brandon:
> - The upper tail of the SAN is a measure of **the nodes that are most correlated given _high_ sustained attention performance (High attention network).** 
> - The lower tail of the SAN is a measure of **the nodes that are most correlated given _low_ sustained attention performance (Low attention network).**

On slide 43's question, the correct answers are both C and D (Continuum of neural and behavioural dysfunction _and_ disorder in sustained attention and cognition). I've updated the version of the slides with notes to reflect this.

## Class 14 - 
03.04.25
[[Inman_Amygdala_In_2023-2.pdf]]
[[Class14_365_2025_notes.pdf]]

### Motivation and Emotion

-  Describe four evolutionarily conserved basic emotional/motivational systems 
- Explain ways in which amygdala and mid-brain dopamine systems play a role guiding motivated attention 
- Evaluate factors that influence individual differences in vulnerability to anxiety, addiction and depression

**Motivation**: The impulse to approach/avoid smth that rewards/punishes
- *Urge towards action*
**Emotion**: Physiological sensations of emotional arousal and subj.v feelings that go w/ them 
- *Subjective experience*

#### Basic brain systems for motivation and emotion
1. Circuits for responding to major life-changing events
	1. Amyg. and basal g. 
2. Organize behav responses
3. Exchg info w fronto-parietal sys important for high order consciousness cogn
4. *Change sensitivities of sensory sys relevant for dealing w those events*

Like dorsal attn network; attune sensory cortices to salient/relevant stim
Best understood, most reliable circuits: "grade-a " emotional sys (Panskepp)
→ ![[Screenshot 2025-03-09 at 12.15.19 PM.png|300]]

#### Amygdala
Hub; connects to many other regns
Tagging what's biologically important and guiding attn/memory 


> [!NOTE]  *Review Inman paper*
> - What are key regions that are nodes in amyg-networks for its function in emtnl percept and mem?
> -  What amygdala functions have been demonstrated by case studies from the last 3 decades? 
> - What is unique about the structure of the amygdala and what does that mean for its multiple functions?
> - How do the authors summarize the role of the amygdala in present perceptions? 
> - List findings from lesion and stimulation studies they use to support this claim. 
> - How do the authors describe the role of the amygdala in prioritizing certain memories? 
> - List findings they use to support this claim 
> - How do the authors claim that the amygdala’s role in perception and memory reflects its unique structure?

Amygdala solves the problem: "How can a limited-capacity information processing system that receives a constant stream of diverse inputs selectively process those inputs that are the most relevant to the goals of the animal?"

Emotional relevance is subj; e.g. depression biases towards neg. stim. 
~Universal = emotionally salient

##### SP
**SP** lost amyg later in life (compared to **SM** who lost in childhood)
- Bilat amyg lesions, R.amyg removed and L damaged due to seizures
- Funny, likeable, avg IQ
- After surgery, participated in many studies by researchers Liz Phelps and Adam Anderson

*Attentional blink (AB)*
- When attending to multiple target stim., takes .5 second to be able to register target 2 immediately following target 1–if T2 follows <.5 second after, we don't see it 
*Emotional sparing/AB sparing*
- Reduced attnl blink when T2 is high in emotional arousal–measure of attnl bias
- Shaped by experience; words/images relating to personal traumatic experiences are more emotionally salient
	- Combat related words for soldiers w/&w/o PTSD, & civilian controls – combat words for those w PTSD activated *visual cortex*

**SP: Impaired Emotional Attention**
- "SP does not have this superpower for detecting emotionally relevant words that the rest of us do. This was true for both positive words and negative words. To rule out the possibility that S. P. was just poor at perception in general, the visual similarity of targets and distractors was manipulated, so that the targets would stand out from distractors to a greater or lesser degree"
	- Q: What kind of attentional process does manipulating visual similarity get at? (bottom up) What attentional network is responsible? (VAN). Like controls, S. P. showed AB sparing for words that were visually easier to perceive -- in contrast to impaired AB sparing for emotional words. The conclusion was that (3) the amygdala influences selective attention for emotional relevance but not perceptual salience

But: *No impairment to **feeling** emotion!*

#### Fear system
**Fear**
- Identifiable threat in the near future
- Strong threat; amyg signals to ANS→ hypothal, heart rate, BP incr.,  etc. 
**Anxiety**
- Overdrive fear system
- Also activates stress response to lesser degree
	- Chronic stress = incr. inflammation, exacerbate illness
- Amyg. more sensitive to threat than reward
- Leads to elevated physiolog. response, sensory processing, memory, rumination
- Attn captured by threatening stim. 
- "Capture followed by avoidance" -- once you notice threatening thing you avoid it
- Can cause attnl bias; misinterp. nonthreatening stim as threatening

Attentional biases cause feedback loops; *avoiding* situations and not learning how to deal w/ them, attn more attuned to threatening stim, rumination incr. anxiety more, further attunes attn





## Class 15 - MT2 Review
https://ubc.ca.panopto.com/Panopto/Pages/Viewer.aspx?id=221e6a33-024d-4546-bc07-b25f0138c969
